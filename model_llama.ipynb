{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "epochs = 5\n",
    "data_dir1 = \"./data/just_car/\"\n",
    "data_dir2 = \"./data/car_trees/\"\n",
    "output_dir = \"model\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        # Get all PNG files in the folder\n",
    "        self.imgs = sorted([f for f in os.listdir(root) if f.endswith('.png')])\n",
    "        # For each image, get corresponding .npy and .json files\n",
    "        self.npy_files = []\n",
    "        self.json_files = []\n",
    "        for png_file in self.imgs:\n",
    "            idx = png_file.replace('rgb_', '').replace('.png', '')\n",
    "            npy_name = f\"bounding_box_2d_tight_{idx}.npy\"\n",
    "            json_name = f\"bounding_box_2d_tight_labels_{idx}.json\"\n",
    "            self.npy_files.append(npy_name)\n",
    "            self.json_files.append(json_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load bounding boxes\n",
    "        npy_path = os.path.join(self.root, self.npy_files[idx])\n",
    "        bboxes = np.load(npy_path)  # shape: (N, 5) => [object_id, x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Load labels\n",
    "        json_path = os.path.join(self.root, self.json_files[idx])\n",
    "        with open(json_path, 'r') as f:\n",
    "            label_dict = json.load(f)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for box in bboxes:\n",
    "            obj_id = int(box[0])\n",
    "            x_min = float(box[1])\n",
    "            y_min = float(box[2])\n",
    "            x_max = float(box[3])\n",
    "            y_max = float(box[4])\n",
    "            # Skip invalid boxes\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                continue\n",
    "\n",
    "            obj_class_name = label_dict.get(str(obj_id), {}).get(\"class\", \"unknown\")\n",
    "            # Only process ground and cars classes\n",
    "            if obj_class_name == \"ground\":\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(\"ground\")\n",
    "            elif obj_class_name == \"cars\":\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(\"car\")\n",
    "\n",
    "        # Create a text description from the detection annotations\n",
    "        if boxes:\n",
    "            description = \"; \".join([f\"{label} at {box}\" for label, box in zip(labels, boxes)])\n",
    "        else:\n",
    "            description = \"No objects detected\"\n",
    "\n",
    "        return img, description\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, texts = zip(*batch)\n",
    "    return list(images), list(texts)\n",
    "\n",
    "# Load meta-llama's vision-language model and its processor from Hugging Face\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset1 = CarDataset(data_dir1)\n",
    "dataset2 = CarDataset(data_dir2)\n",
    "\n",
    "data_loader1 = DataLoader(dataset1, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader2 = DataLoader(dataset2, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Use an optimizer over all trainable parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Use dataset1 for epochs 1 and 2; use dataset2 for later epochs\n",
    "    if epoch < 2:\n",
    "        current_loader = data_loader1\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]: Using dataset1\")\n",
    "    else:\n",
    "        current_loader = data_loader2\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]: Using dataset2\")\n",
    "\n",
    "    len_dataloader = len(current_loader)\n",
    "    for i, (imgs, texts) in enumerate(current_loader):\n",
    "        # The processor handles any necessary image pre-processing and tokenizes text.\n",
    "        inputs = processor(images=imgs, text=texts, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # For a generative model, we pass the input_ids as labels.\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"  Batch [{i+1}/{len_dataloader}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the fine-tuned model and processor\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
