{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56670aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in[0]: import sys\n",
    "sys.path.append('./detr')  # Add this line before other imports\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import json\n",
    "import shutil\n",
    "from detr.models.matcher import HungarianMatcher\n",
    "from detr.models.detr import SetCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9396ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "num_classes = 2  # Two object classes: ground and cars\n",
    "data_dir1 = \"./data/just_car/\"\n",
    "data_dir2 = \"./data/car_trees/\"\n",
    "output_file = \"my_model_3_detr.pth\"\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764124e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Get all PNG files in the folder\n",
    "        self.imgs = sorted([f for f in os.listdir(root) if f.endswith('.png')])\n",
    "\n",
    "        # For each image, get corresponding .npy and .json files\n",
    "        self.npy_files = []\n",
    "        self.json_files = []\n",
    "        for png_file in self.imgs:\n",
    "            idx = png_file.replace('rgb_', '').replace('.png', '')\n",
    "            npy_name = f\"bounding_box_2d_tight_{idx}.npy\"\n",
    "            json_name = f\"bounding_box_2d_tight_labels_{idx}.json\"\n",
    "            self.npy_files.append(npy_name)\n",
    "            self.json_files.append(json_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size  # Get image dimensions\n",
    "\n",
    "        # Load bounding boxes\n",
    "        npy_path = os.path.join(self.root, self.npy_files[idx])\n",
    "        bboxes = np.load(npy_path)  # shape: (N, 5) => [object_id, x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Load labels\n",
    "        json_path = os.path.join(self.root, self.json_files[idx])\n",
    "        with open(json_path, 'r') as f:\n",
    "            label_dict = json.load(f)\n",
    "\n",
    "        # Parse bounding boxes & labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for box in bboxes:\n",
    "            obj_id = int(box[0])\n",
    "            x_min = float(box[1])\n",
    "            y_min = float(box[2])\n",
    "            x_max = float(box[3])\n",
    "            y_max = float(box[4])\n",
    "            if x_max <= x_min or y_max <= y_min:\n",
    "                continue\n",
    "\n",
    "            obj_class_name = label_dict.get(str(obj_id), {}).get(\"class\", \"unknown\")\n",
    "            if obj_class_name == \"ground\":\n",
    "                class_label = 0\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(class_label)\n",
    "            elif obj_class_name == \"cars\":\n",
    "                class_label = 1\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(class_label)\n",
    "\n",
    "        # Convert lists to torch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Normalize boxes to [0, 1] using image dimensions\n",
    "        if boxes.shape[0] > 0:\n",
    "            boxes[:, [0, 2]] /= w\n",
    "            boxes[:, [1, 3]] /= h\n",
    "            areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": areas\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    # Normalize images using ImageNet statistics (DETR was trained with these)\n",
    "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1338cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(num_classes):\n",
    "    # DETR expects the number of classes to include an extra \"no-object\" class\n",
    "    model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "    hidden_dim = model.transformer.d_model\n",
    "    model.class_embed = torch.nn.Linear(hidden_dim, num_classes + 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b49ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher(cost_class=1, cost_bbox=5, cost_giou=2)\n",
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "\n",
    "criterion = SetCriterion(\n",
    "    num_classes=num_classes,\n",
    "    matcher=matcher,\n",
    "    weight_dict=weight_dict,\n",
    "    eos_coef=0.1,\n",
    "    losses=losses\n",
    ")\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "dataset1 = CarDataset(data_dir1, get_transform(train=True))\n",
    "dataset2 = CarDataset(data_dir2, get_transform(train=True))\n",
    "\n",
    "data_loader1 = torch.utils.data.DataLoader(\n",
    "    dataset1, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader2 = torch.utils.data.DataLoader(\n",
    "    dataset2, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and move model to the device\n",
    "model = create_model(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5315a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use AdamW optimizer as recommended for DETR\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < 2:\n",
    "        current_loader = data_loader1\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]: Using dataset1\")\n",
    "    else:\n",
    "        current_loader = data_loader2\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]: Using dataset2\")\n",
    "\n",
    "    len_dataloader = len(current_loader)\n",
    "    for i, (imgs, targets) in enumerate(current_loader):\n",
    "        imgs = torch.stack(imgs).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        # Use the separate criterion\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"  Batch [{i+1}/{len_dataloader}], Loss: {losses.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aeb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model's state dictionary\n",
    "torch.save(model, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
